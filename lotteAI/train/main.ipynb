{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7c1567-9f99-4d80-9f14-86ccb2f463dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2> 0. 데이터 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357b746f-896c-4abd-a50a-424b507c06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /Users/sungheui/코딩, /Users/sungheui/코딩.zip or /Users/sungheui/코딩.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip /Users/sungheui/코딩 테스트/ArtClassify/train.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbcba67f-0f8b-49fa-9908-1f9ed8293e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sungheui/sh/lotteAI'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0598875a-f8e9-4265-86fb-8686d30e6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps 사용 가능 여부: True\n",
      "mps 지원 환경 여부: True\n"
     ]
    }
   ],
   "source": [
    "#환경 설정 확인하기\n",
    "import torch\n",
    "print(f\"mps 사용 가능 여부: {torch.backends.mps.is_available()}\")\n",
    "print(f\"mps 지원 환경 여부: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "#GPU check DEVICE\n",
    "# print(f'PyTorch Version : [{torch.__version__}]')\n",
    "# device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'Device : [{device}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b274d6-e42d-4844-ae4a-01441c0f6961",
   "metadata": {},
   "source": [
    "<h3> 2. 라이브러리 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11deab99-8c0b-4277-b768-d3d3000f6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902df84-0de7-4854-9e65-a52982dc3261",
   "metadata": {},
   "source": [
    "<h3> 3. Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9360ae75-537b-464e-9733-9ae49ef4ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version : [2.1.2]\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "#CONFIG\n",
    "torch.manual_seed(128)\n",
    "BATCH_SIZE= 2\n",
    "EPOCHS=40\n",
    "LEARNING_RATE=1e-3\n",
    "print(f'PyTorch Version : [{torch.__version__}]')\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90febacd-53ed-49f4-ad0e-dd27018d1074",
   "metadata": {},
   "source": [
    "<h3> 4. Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "283ae72c-6bf4-4015-b058-76175221b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb;pdb.set_trace() \n",
    "class LotteDataset(Dataset):\n",
    "  def __init__(self, data_root, train_mode, transform=None):\n",
    "    super(LotteDataset, self).__init__()\n",
    "    self.train_mode=train_mode\n",
    "    self.transform = transform\n",
    "    self.label2idx = {'dog': 0,\n",
    "             'elephant': 1,\n",
    "             'giraffe': 2,\n",
    "             'guitar': 3,\n",
    "             'horse': 4,\n",
    "             'house': 5,\n",
    "             'person': 6}\n",
    "   \n",
    "    if self.train_mode==False:\n",
    "        \n",
    "        self.img_list= []\n",
    "        img_list = []\n",
    "        for file in os.listdir(data_root):\n",
    "            file_root = os.path.join(data_root, file)\n",
    "            for data in os.listdir(file_root):\n",
    "                data_path = os.path.join(file_root, data)\n",
    "                img_list.append(data_path)\n",
    "        self.img_list = sorted(img_list)\n",
    "        \n",
    "    else: #학습할 때 \n",
    "        self.img_list = []\n",
    "        self.label_list=[]\n",
    "        img_list = []\n",
    "        for file in os.listdir(data_root):\n",
    "            file_root = os.path.join(data_root, file)\n",
    "            for data in os.listdir(file_root):\n",
    "                data_path = os.path.join(file_root, data)\n",
    "                img_list.append(data_path)\n",
    "        self.img_list = sorted(img_list)\n",
    "\n",
    "        for label in img_list:\n",
    "            label = label.split('/')[3]\n",
    "            self.label_list.append(self.label2idx[label]) \n",
    "            \n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.img_list)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img_path = self.img_list[index]\n",
    "    \n",
    "    if self.train_mode:\n",
    "        label = self.label_list[index]\n",
    "     \n",
    "\n",
    "    # Image Loading\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    if self.train_mode:\n",
    "      return img,label\n",
    "    else:\n",
    "      return img\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     data = LotteDataset('./dataset/train',train_mode=True,transform=train_transforms)\n",
    "#     q= data.__getitem__(1)\n",
    "#     print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80e6a86f-f05d-411c-9fe2-50b457062d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapTransform(Dataset):\n",
    "    def __init__(self, dataset, transform, train_mode):\n",
    "        self.dataset = dataset\n",
    "        self.transform=transform\n",
    "        self.train_mode=train_mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train_mode:\n",
    "          return self.transform(self.dataset[index][0]), self.dataset[index][1]\n",
    "        else:\n",
    "          return self.transform(self.dataset[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f2a3160-bc43-464a-a228-558009425e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trian Augmentation \n",
    "train_transforms=transforms.Compose([\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(brightness=(1,1.1)),\n",
    "        transforms.ColorJitter(contrast=0.1), \n",
    "        transforms.ColorJitter(saturation=0.1),\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10,fill=255),\n",
    "        transforms.RandomCrop((224,224)),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "#Test Augmentation \n",
    "test_transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Resize((224,224)),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0855238-d1d0-42dc-9735-a0dc28c60739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#그냥 train-test -> validation x\n",
    "train_data=LotteDataset('./dataset/train',train_mode=True)\n",
    "test_data=LotteDataset('./dataset/test',train_mode=False)\n",
    "\n",
    "trans_train_data=MapTransform(train_data,train_transforms,train_mode=True)\n",
    "trans_test_data=MapTransform(test_data,test_transforms,train_mode=False)\n",
    "\n",
    "train_iter=DataLoader(trans_train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "test_iter=DataLoader(trans_test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11fa3dc9-fef5-4ca8-bd23-ff0a6a8226a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cross-validation 할때 이걸로 \n",
    "# all_data=LotteDataset('./dataset/train',train_mode=True,transform=train_transforms)\n",
    "# test_data=LotteDataset('./dataset/test',train_mode=False,transform=test_transforms)\n",
    "# test_iter=DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "804f8af7-23d4-451a-89df-be9da3d30fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n",
      "351\n"
     ]
    }
   ],
   "source": [
    "print(len(trans_train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24b378-8efa-4b8f-93a6-1ff2accd2cdc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h3> 5. Model Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af7a123f-6f8b-4280-9766-5a21244192ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#현재 디렉토리 찾기 \n",
    "# os.getcwd()\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train.tt import VIT2\n",
    "\n",
    "model = VIT2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ada36fd-bcf4-463c-ae72-e183d177db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_embeddings\n",
      "model.head.weight\n",
      "model.head.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad==True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3541e5fb-b2fe-49d5-87c3-3a7c00427110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss label smooth\n",
    "def loss_fn(outputs, targets):\n",
    "    if len(targets.shape) == 1:\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    else:\n",
    "        return torch.mean(torch.sum(-targets * F.log_softmax(outputs, dim=1), dim=1))\n",
    "\n",
    "def label_smooth_loss_fn(outputs, targets, epsilon=0.1):\n",
    "    onehot = F.one_hot(targets, 1000).float().to(device)\n",
    "    targets = (1 - epsilon) * onehot + torch.ones(onehot.shape).to(device) * epsilon / 1000\n",
    "    return loss_fn(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb522f-e58f-48d9-a0b1-b5663cc8422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(Model,data_iter,loss):\n",
    "    with torch.no_grad():\n",
    "      Model.eval()\n",
    "      n_total, n_correct = 0,0\n",
    "      loss_val_sum=0\n",
    "      print(\"(Train or Validation) Data Testing....\\n\")\n",
    "      for imgs, labels in iter(data_iter):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        model_pred=Model(imgs)\n",
    "\n",
    "        loss_out=loss(model_pred,labels)\n",
    "        loss_val_sum+=loss_out\n",
    "\n",
    "        _, y_pred=torch.max(model_pred.data,1)\n",
    "        n_correct+=(y_pred==labels).sum().item()\n",
    "        n_total+=imgs.size(0)\n",
    "      val_accr=(n_correct/n_total)\n",
    "      Model.train()\n",
    "      loss_val_avg=loss_val_sum/len(data_iter)\n",
    "    print(\"Testing Done.\\n\")\n",
    "    return val_accr,loss_val_avg\n",
    "    \n",
    "def get_submission(Model,data_iter,epoch,fold):\n",
    "  with torch.no_grad():\n",
    "    Model.eval()\n",
    "    pred_label=[]\n",
    "    print(\"Final Testing....\\n\")\n",
    "    for imgs in iter(test_iter):\n",
    "      model_pred=Model(imgs.to(device))\n",
    "\n",
    "      _, y_pred=torch.max(model_pred.data,1)\n",
    "      pred_label.extend(y_pred.tolist())\n",
    "\n",
    "  Model.train()\n",
    "\n",
    "  submission = pd.read_csv('./dataset/test_answer_sample_.csv', encoding = 'utf-8')\n",
    "  submission['prediction'] = pred_label\n",
    "  submission.to_csv('./submission_50Resnext'+str(fold)+'_'+str(epoch)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fd2fb-9480-4678-9610-6c87abbfb627",
   "metadata": {},
   "source": [
    "<h3> Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c06fd-7acb-474e-8637-cdf05d8703ee",
   "metadata": {},
   "source": [
    "<h2> cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4f4ba53-bfad-420b-9656-b54d3b5a0cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folds=StratifiedKFold(n_splits=4,shuffle=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "##Loss Function 택1\n",
    "# loss=label_smooth_loss_fn\n",
    "loss = loss_fn \n",
    "f = open(\"./vitPrompt_trainlog2.txt\", 'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8b922b0-dbff-4dca-82ba-82bb19a4013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "::::::::: Fold : 0 :::::::::::\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model_pred\u001b[38;5;241m=\u001b[39mmodel(imgs)\n\u001b[1;32m     24\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m loss(model_pred, labels)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     27\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(BATCH_SIZE)\n",
    "for current_fold,(train_idx, vali_idx) in enumerate(folds.split(all_data,all_data.label_list)):\n",
    "    \n",
    "  train_data=torch.utils.data.Subset(all_data,train_idx)\n",
    "  vali_data=torch.utils.data.Subset(all_data,vali_idx)\n",
    "    \n",
    "  train_iter=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "  vali_iter=DataLoader(vali_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "\n",
    "  print(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n",
    "  f.write(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n",
    "  # model.train()\n",
    "  prev_vali_loss=100\n",
    "  best_vali_loss=100\n",
    "  flag=0\n",
    "  over_check=0\n",
    "  for epoch in range(EPOCHS) :\n",
    "    # if epoch==3:\n",
    "    #   freeze(Model,-3)\n",
    "    loss_val_sum=0\n",
    "    for imgs, labels in iter(train_iter):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        model_pred=model(imgs)\n",
    "        loss_out = loss(model_pred, labels)\n",
    "        scaler.scale(loss_out).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_val_sum+=loss_out\n",
    "    loss_val_avg=loss_val_sum/len(train_iter)\n",
    "    vali_accr,vali_loss=func_eval(model,vali_iter,loss_fn)\n",
    "    if epoch>=0:\n",
    "      get_submission(model,test_iter,epoch,current_fold)\n",
    "      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      print(\"Model Save....\\n\")\n",
    "      torch.save({'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict()}, './checkpoint/'+str(current_fold)+'_VPT_epoch_'+str(epoch)+'.tar')\n",
    "    else:\n",
    "      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "    scheduler.step(vali_loss) # LR Scheduler\n",
    "\n",
    "    if prev_vali_loss<vali_loss: #Stop Train\n",
    "      flag+=1\n",
    "      if flag==10 : \n",
    "        print(\"Stop Training...\\n\")\n",
    "        break\n",
    "    if best_vali_loss>vali_loss:\n",
    "      flag=0\n",
    "      best_vali_loss=vali_loss\n",
    "    prev_vali_loss=vali_loss\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85822bcc-e0a2-4e77-92fd-7dca1b26c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f98132bd-209c-4d88-94bb-71f3682c7cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 2.2489,  2.2489,  2.2489,  ..., -0.4910, -0.0598,  0.2982],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ..., -0.5508, -0.0687, -0.1299],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ..., -0.2422, -0.2603, -0.1485],\n",
      "          ...,\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],\n",
      "\n",
      "         [[ 2.4286,  2.4286,  2.4286,  ..., -0.4640,  0.0993,  0.5209],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ..., -0.5219,  0.0903,  0.0832],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ..., -0.1579, -0.1039,  0.0618],\n",
      "          ...,\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],\n",
      "\n",
      "         [[ 2.6400,  2.6400,  2.6400,  ..., -0.6255, -0.0623,  0.4076],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ..., -0.6822, -0.0714, -0.0784],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ..., -0.3359, -0.2652, -0.0997],\n",
      "          ...,\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          ...,\n",
      "          [ 1.2389,  1.2557,  1.2557,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 1.2389,  1.2557,  1.2557,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 1.2389,  1.2557,  1.2557,  ...,  2.2489,  2.2489,  2.2489]],\n",
      "\n",
      "         [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          ...,\n",
      "          [ 1.2209,  1.2381,  1.2381,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 1.2209,  1.2381,  1.2381,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 1.2209,  1.2381,  1.2381,  ...,  2.4286,  2.4286,  2.4286]],\n",
      "\n",
      "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          ...,\n",
      "          [ 1.3157,  1.3328,  1.3328,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 1.3157,  1.3328,  1.3328,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 1.3157,  1.3328,  1.3328,  ...,  2.6400,  2.6400,  2.6400]]]]), tensor([1, 5])]\n"
     ]
    }
   ],
   "source": [
    "dataloader_iter = iter(train_iter)\n",
    "batch = next(dataloader_iter)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0ab5066e-b5ab-46be-951b-bb832e229595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4289a-6878-47b9-b007-980978656a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
