{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7c1567-9f99-4d80-9f14-86ccb2f463dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2> 0. 데이터 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357b746f-896c-4abd-a50a-424b507c06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /Users/sungheui/코딩, /Users/sungheui/코딩.zip or /Users/sungheui/코딩.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip /Users/sungheui/코딩 테스트/ArtClassify/train.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcba67f-0f8b-49fa-9908-1f9ed8293e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/sungheui/sh/lotteAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0598875a-f8e9-4265-86fb-8686d30e6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps 사용 가능 여부: True\n",
      "mps 지원 환경 여부: True\n"
     ]
    }
   ],
   "source": [
    "#환경 설정 확인하기\n",
    "import torch\n",
    "print(f\"mps 사용 가능 여부: {torch.backends.mps.is_available()}\")\n",
    "print(f\"mps 지원 환경 여부: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "#GPU check DEVICE\n",
    "# print(f'PyTorch Version : [{torch.__version__}]')\n",
    "# device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'Device : [{device}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b274d6-e42d-4844-ae4a-01441c0f6961",
   "metadata": {},
   "source": [
    "<h3> 2. 라이브러리 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "11deab99-8c0b-4277-b768-d3d3000f6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902df84-0de7-4854-9e65-a52982dc3261",
   "metadata": {},
   "source": [
    "<h3> 3. Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9360ae75-537b-464e-9733-9ae49ef4ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version : [2.1.2]\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "#CONFIG\n",
    "torch.manual_seed(128)\n",
    "BATCH_SIZE= 2\n",
    "EPOCHS=40\n",
    "LEARNING_RATE=1e-3\n",
    "print(f'PyTorch Version : [{torch.__version__}]')\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90febacd-53ed-49f4-ad0e-dd27018d1074",
   "metadata": {},
   "source": [
    "<h3> 4. Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "283ae72c-6bf4-4015-b058-76175221b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb;pdb.set_trace() \n",
    "class LotteDataset(Dataset):\n",
    "  def __init__(self, data_root, train_mode, transform=None):\n",
    "    super(LotteDataset, self).__init__()\n",
    "    self.train_mode=train_mode\n",
    "    self.transform = transform\n",
    "    self.label2idx = {'dog': 0,\n",
    "             'elephant': 1,\n",
    "             'giraffe': 2,\n",
    "             'guitar': 3,\n",
    "             'horse': 4,\n",
    "             'house': 5,\n",
    "             'person': 6}\n",
    "   \n",
    "    if self.train_mode==False:\n",
    "        \n",
    "        self.img_list= []\n",
    "        img_list = []\n",
    "        for file in os.listdir(data_root):\n",
    "            file_root = os.path.join(data_root, file)\n",
    "            for data in os.listdir(file_root):\n",
    "                if '.jpg' in data:\n",
    "                    data_path = os.path.join(file_root, data)\n",
    "                    img_list.append(data_path)\n",
    "                \n",
    "        self.img_list = sorted(img_list)\n",
    "\n",
    "    else: #학습할 때 \n",
    "        self.img_list = []\n",
    "        self.label_list=[]\n",
    "        img_list = []\n",
    "        for file in os.listdir(data_root):\n",
    "            file_root = os.path.join(data_root, file)\n",
    "            for data in os.listdir(file_root):\n",
    "                if '.jpg' in data:\n",
    "                    data_path = os.path.join(file_root, data)\n",
    "                    img_list.append(data_path)\n",
    "\n",
    "        self.img_list = sorted(img_list)\n",
    "       \n",
    "        for label in self.img_list:\n",
    "            label = label.split('/')[-2]\n",
    "           \n",
    "            self.label_list.append(self.label2idx[label]) \n",
    "            \n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.img_list)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img_path = self.img_list[index]\n",
    "\n",
    "    #Image, Label Loading\n",
    "    if self.train_mode:\n",
    "        label = self.label_list[index]\n",
    "     \n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    #Augmentation  \n",
    "    if self.transform: \n",
    "      img = self.transform(img)\n",
    "\n",
    "    if self.train_mode:\n",
    "      return img,label\n",
    "    else:\n",
    "      return img\n",
    "\n",
    "# if __name__=='__main__':\n",
    "    # data = LotteDataset('./dataset/test',train_mode=False,transform=train_transforms)\n",
    "    # data = LotteDataset('./dataset/train',train_mode=True,transform=train_transforms)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # q= data.__getitem__(1)\n",
    "    # print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80e6a86f-f05d-411c-9fe2-50b457062d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapTransform(Dataset):\n",
    "    def __init__(self, dataset, transform, train_mode):\n",
    "        self.dataset = dataset\n",
    "        self.transform=transform\n",
    "        self.train_mode=train_mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train_mode:\n",
    "          return self.transform(self.dataset[index][0]), self.dataset[index][1]\n",
    "        else:\n",
    "          return self.transform(self.dataset[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1f2a3160-bc43-464a-a228-558009425e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Augmentation \n",
    "train_transforms=transforms.Compose([\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(brightness=(1,1.1)),\n",
    "        transforms.ColorJitter(contrast=0.1), \n",
    "        transforms.ColorJitter(saturation=0.1),\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10,fill=255),\n",
    "        transforms.RandomCrop((224,224)), #이미지 사이즈 설정 \n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "#Test Augmentation \n",
    "test_transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Resize((224,224)),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f0855238-d1d0-42dc-9735-a0dc28c60739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#그냥 train-test -> validation x\n",
    "train_data=LotteDataset('./dataset/train',train_mode=True)\n",
    "test_data=LotteDataset('./dataset/test',train_mode=False)\n",
    "\n",
    "trans_train_data=MapTransform(train_data,train_transforms,train_mode=True)\n",
    "trans_test_data=MapTransform(test_data,test_transforms,train_mode=False)\n",
    "\n",
    "train_iter=DataLoader(trans_train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "test_iter=DataLoader(trans_test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11fa3dc9-fef5-4ca8-bd23-ff0a6a8226a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cross-validation 할때 이걸로 \n",
    "# all_data=LotteDataset('./dataset/train',train_mode=True,transform=train_transforms)\n",
    "# test_data=LotteDataset('./dataset/test',train_mode=False,transform=test_transforms)\n",
    "# test_iter=DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "804f8af7-23d4-451a-89df-be9da3d30fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "print(len(trans_train_data)) #1698\n",
    "print(len(test_data)) #350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24b378-8efa-4b8f-93a6-1ff2accd2cdc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h3> 5. Model Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "af7a123f-6f8b-4280-9766-5a21244192ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIT_Base(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       "  (prompt_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#현재 디렉토리 찾기 \n",
    "# os.getcwd()\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from train.tt import VIT2\n",
    "# model = VIT2().to(device)\n",
    "\n",
    "\n",
    "# from train.VIT_Small import VIT_Small\n",
    "# model = VIT_Small().to(device)\n",
    "# model.train()\n",
    "\n",
    "from train.VIT_Base import VIT_Base\n",
    "model = VIT_Base().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5ada36fd-bcf4-463c-ae72-e183d177db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_embeddings\n",
      "model.head.weight\n",
      "model.head.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad==True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3541e5fb-b2fe-49d5-87c3-3a7c00427110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss label smooth\n",
    "def loss_fn(outputs, targets):\n",
    "    if len(targets.shape) == 1:\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    else:\n",
    "        return torch.mean(torch.sum(-targets * F.log_softmax(outputs, dim=1), dim=1))\n",
    "\n",
    "def label_smooth_loss_fn(outputs, targets, epsilon=0.1):\n",
    "    onehot = F.one_hot(targets, 10).float().to(device)\n",
    "    targets = (1 - epsilon) * onehot + torch.ones(onehot.shape).to(device) * epsilon / 1000\n",
    "    return loss_fn(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ceb522f-e58f-48d9-a0b1-b5663cc8422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(Model,data_iter,loss):\n",
    "    with torch.no_grad():\n",
    "      Model.eval()\n",
    "      n_total, n_correct = 0,0\n",
    "      loss_val_sum=0\n",
    "      print(\"(Train or Validation) Data Testing....\\n\")\n",
    "      for imgs, labels in iter(data_iter):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        model_pred=Model(imgs)\n",
    "\n",
    "        loss_out=loss(model_pred,labels)\n",
    "        loss_val_sum+=loss_out\n",
    "\n",
    "        _, y_pred=torch.max(model_pred.data,1)\n",
    "        n_correct+=(y_pred==labels).sum().item()\n",
    "        n_total+=imgs.size(0)\n",
    "      val_accr=(n_correct/n_total)\n",
    "      Model.train()\n",
    "      loss_val_avg=loss_val_sum/len(data_iter)\n",
    "    print(\"Testing Done.\\n\")\n",
    "    return val_accr,loss_val_avg\n",
    "    \n",
    "def get_submission(Model,data_iter,epoch,fold=None):\n",
    "  with torch.no_grad():\n",
    "    Model.eval()\n",
    "    pred_label=[]\n",
    "    print(\"Final Testing....\\n\")\n",
    "    for imgs in iter(test_iter):\n",
    "      model_pred=Model(imgs.to(device))\n",
    "\n",
    "      _, y_pred=torch.max(model_pred.data,1)\n",
    "      pred_label.extend(y_pred.tolist())\n",
    "\n",
    "  Model.train()\n",
    "\n",
    "  ##Cross-validation##\n",
    "  # submission = pd.read_csv('./dataset/test_answer_sample_.csv', encoding = 'utf-8')\n",
    "  # submission['prediction'] = pred_label\n",
    "  # submission.to_csv('./submission_VIT_small'+str(fold)+'_'+str(epoch)+'.csv', index = False)\n",
    "\n",
    "  ##TRAIN-TEST##\n",
    "  submission = pd.read_csv('./dataset/test_answer_sample_.csv', encoding = 'utf-8') #index_col = 0\n",
    "  submission['answer value'] = pred_label\n",
    "  submission.to_csv('./submission_VIT_base'+'_'+str(epoch)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fd2fb-9480-4678-9610-6c87abbfb627",
   "metadata": {},
   "source": [
    "<h3> Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c06fd-7acb-474e-8637-cdf05d8703ee",
   "metadata": {},
   "source": [
    "<h2> cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f4f4ba53-bfad-420b-9656-b54d3b5a0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds=StratifiedKFold(n_splits=4,shuffle=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "##Loss Function 택1\n",
    "# loss=label_smooth_loss_fn\n",
    "loss = loss_fn \n",
    "f = open(\"./vit_base.txt\", 'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838a81d-df0b-4c1b-b717-4da86b09c8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ae4b9e5f56486ab42541639092ef36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS) :\n",
    "    \n",
    "  loss_val_sum=0\n",
    "  visual_loss_sum=0 # 확인용 Train Loss\n",
    "\n",
    "    \n",
    "  for imgs, labels in tqdm(iter(train_iter)):\n",
    "    # Cut mix P=0.5\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    # print(imgs.shape)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.imsave('1.png', imgs[0][0].detach().cpu())\n",
    "    # plt.imsave('2.png', imgs[1][0].detach().cpu())\n",
    "    # print(labels[0],labels[1] ) [batch,1]\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_pred=model(imgs)\n",
    "    # print('mo',model_pred.shape) #[2,10] \n",
    "    # print('la',labels.shape)\n",
    "    # print(labels[0])\n",
    "    \n",
    "\n",
    "\n",
    "    # Label Smoothing\n",
    "    loss_out = loss(model_pred, labels) \n",
    "    # 확인용 Train Loss\n",
    "    normal_loss_out=loss_fn(model_pred.clone().detach(), labels.clone().detach()) \n",
    "    scaler.scale(loss_out).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    loss_val_sum+=loss_out\n",
    "    visual_loss_sum+=normal_loss_out\n",
    "\n",
    "  loss_val_avg=loss_val_sum/len(train_iter)\n",
    "  visual_loss_avg=visual_loss_sum/len(train_iter)\n",
    "  get_submission(model,test_iter,epoch)\n",
    "\n",
    "  print(\"epoch:[%d] train loss smooth:[%.5f] train normal loss:[%.5f]\\n\"%(epoch,loss_val_avg,visual_loss_avg))\n",
    "  f.write(\"epoch:[%d] train loss smooth:[%.5f] train normal loss:[%.5f]\\n\"%(epoch,loss_val_avg,visual_loss_avg))\n",
    "  print(\"Model Save....\\n\")\n",
    "  torch.save({'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict()}, './checkpoint_vit/ViT_epoch_'+str(epoch)+'.tar')\n",
    "  scheduler.step(loss_val_avg) # LR Scheduler\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8b922b0-dbff-4dca-82ba-82bb19a4013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(BATCH_SIZE)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_fold,(train_idx, vali_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds\u001b[38;5;241m.\u001b[39msplit(\u001b[43mall_data\u001b[49m,all_data\u001b[38;5;241m.\u001b[39mlabel_list)):\n\u001b[1;32m      4\u001b[0m   train_data\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(all_data,train_idx)\n\u001b[1;32m      5\u001b[0m   vali_data\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(all_data,vali_idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "##CROSS-VALIDATION \n",
    "print(BATCH_SIZE)\n",
    "for current_fold,(train_idx, vali_idx) in enumerate(folds.split(all_data,all_data.label_list)):\n",
    "    \n",
    "  train_data=torch.utils.data.Subset(all_data,train_idx)\n",
    "  vali_data=torch.utils.data.Subset(all_data,vali_idx)\n",
    "    \n",
    "  train_iter=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "  vali_iter=DataLoader(vali_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "\n",
    "  print(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n",
    "  f.write(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n",
    "  # model.train()\n",
    "  prev_vali_loss=100\n",
    "  best_vali_loss=100\n",
    "  flag=0\n",
    "  over_check=0\n",
    "  for epoch in range(EPOCHS) :\n",
    "    # if epoch==3:\n",
    "    #   freeze(Model,-3)\n",
    "    loss_val_sum=0\n",
    "    for imgs, labels in iter(train_iter):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        model_pred=model(imgs)\n",
    "        loss_out = loss(model_pred, labels)\n",
    "        scaler.scale(loss_out).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_val_sum+=loss_out\n",
    "    loss_val_avg=loss_val_sum/len(train_iter)\n",
    "    vali_accr,vali_loss=func_eval(model,vali_iter,loss_fn)\n",
    "    if epoch>=0:\n",
    "      get_submission(model,test_iter,epoch,current_fold)\n",
    "      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      print(\"Model Save....\\n\")\n",
    "      torch.save({'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict()}, './checkpoint/'+str(current_fold)+'_VPT_epoch_'+str(epoch)+'.tar')\n",
    "    else:\n",
    "      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n",
    "    scheduler.step(vali_loss) # LR Scheduler\n",
    "\n",
    "    if prev_vali_loss<vali_loss: #Stop Train\n",
    "      flag+=1\n",
    "      if flag==10 : \n",
    "        print(\"Stop Training...\\n\")\n",
    "        break\n",
    "    if best_vali_loss>vali_loss:\n",
    "      flag=0\n",
    "      best_vali_loss=vali_loss\n",
    "    prev_vali_loss=vali_loss\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85822bcc-e0a2-4e77-92fd-7dca1b26c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98132bd-209c-4d88-94bb-71f3682c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_iter = iter(train_iter)\n",
    "# batch = next(dataloader_iter)\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4289a-6878-47b9-b007-980978656a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
